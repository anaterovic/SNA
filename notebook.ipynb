{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.load_data import load_postings, load_votes, get_first_contact_df, subset_users\n",
    "\n",
    "# reload imports jupyter magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votes loaded\n",
      "Postings loaded\n"
     ]
    }
   ],
   "source": [
    "votes = load_votes(\"input/\")\n",
    "postings = load_postings(\"input/\")\n",
    "votes = votes.sort_values(\"VoteCreatedAt\")\n",
    "postings[\"num_interactions\"] = postings.groupby(\"UserCommunityName\")[\"PostingCreatedAt\"].cumcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to have the following users\n",
    "- users that interacted with only a few posts\n",
    "- users that interacted with many posts, but in few days creating a skewed distribution\n",
    "\n",
    "Therefore we set a threshold on the number of days a user has to interact minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering users that interacted at the middle of the interval. Possible future extension: pick time interval instead of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.10/site-packages/numpy/core/_methods.py:44: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior.  In a future version these will be considered non-comparable.Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  return umr_minimum(a, axis, None, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "user_selection = subset_users(votes, postings, \"both\", num_days_min=30, firt_interaction_middle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2630704409233873"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_selection.nunique() /  pd.concat([votes[\"UserCommunityName\"], postings[\"UserCommunityName\"]]).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a notion of similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comment article similarity\n",
    "#### \"Users are similar when they comments under the same articles (or articles of the same ressort)\"\n",
    "\n",
    "Todo: aggregate postings based on author and target (e.g., article, channel, ressort). Either count or just stay binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_postings = postings[postings[\"UserCommunityName\"].isin(user_selection)].sort_values(\"PostingCreatedAt\")\n",
    "selected_postings[\"UserCommunityName\"] = \"user_\" + selected_postings[\"UserCommunityName\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# one can adapt this with e.g. ressort instead of article\n",
    "\n",
    "def create_graph(df, article_or_ressort, user=\"UserCommunityName\"):\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(df[article_or_ressort].unique())\n",
    "    graph.add_nodes_from(df[user].unique())\n",
    "    graph.add_edges_from(list(map(tuple, df[[article_or_ressort, user]].drop_duplicates().values)))\n",
    "    graph = graph.to_undirected()\n",
    "    return graph\n",
    "\n",
    "def compute_overlap(graph, df, article_or_ressort,verbose=False):\n",
    "    uu_overlap = {}\n",
    "    article_ids = df[article_or_ressort].unique()\n",
    "    for idx, article in enumerate(article_ids):\n",
    "        if verbose: print(round((idx/len(article_ids))*100), \"%\", end=\"\\r\")\n",
    "        users_commented =list(graph.neighbors(article))\n",
    "        for uu_tuple in itertools.product(users_commented, users_commented):\n",
    "            if uu_tuple[0] != uu_tuple[1]:\n",
    "                if uu_tuple[0] > uu_tuple[1]:\n",
    "                    uu_tuple = (uu_tuple[1], uu_tuple[0])\n",
    "                if uu_tuple in uu_overlap:\n",
    "                    uu_overlap[uu_tuple] += 1\n",
    "                else:\n",
    "                    uu_overlap[uu_tuple] = 1\n",
    "                    \n",
    "    return uu_overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_lookup_df(df, article_or_ressort):\n",
    "    user_num_articles = df[[\"UserCommunityName\", article_or_ressort]].drop_duplicates()\\\n",
    "        .groupby([\"UserCommunityName\"]).size().to_frame()\n",
    "    # make dict of users and the number of articles they commented on\n",
    "    user_num_articles = dict(zip(user_num_articles.index, user_num_articles[0]))\n",
    "    return user_num_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the amount of common articles two users posted on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(uu_overlap, user_num_articles, chunckIdx):\n",
    "    similarities = []\n",
    "    for uu_tuple in uu_overlap.keys():\n",
    "        overlap = uu_overlap[uu_tuple]\n",
    "        try:\n",
    "            union = user_num_articles[uu_tuple[0]] + user_num_articles[uu_tuple[1]]\n",
    "        except:\n",
    "            print(uu_tuple)\n",
    "        similarities += [[uu_tuple[0],uu_tuple[1], overlap / union]]\n",
    "    return pd.DataFrame(similarities, columns=[\"A\", \"B\", f\"Similarity_{chunckIdx}\"]).set_index([\"A\", \"B\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_similarity() missing 1 required positional argument: 'chunckIdx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m uu_overlap_article \u001b[39m=\u001b[39m compute_overlap(graph_article, selected_postings, \u001b[39m\"\u001b[39m\u001b[39mID_Article\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m user_num_articles \u001b[39m=\u001b[39m user_lookup_df(selected_postings, \u001b[39m\"\u001b[39m\u001b[39mID_Article\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m similarity_table_article \u001b[39m=\u001b[39m compute_similarity(uu_overlap_article, user_num_articles)\n\u001b[1;32m      5\u001b[0m similarity_table_article\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39moutput/similarity_table_article_all.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[39mdel\u001b[39;00m similarity_table_article, graph_article, uu_overlap_article, user_num_articles\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_similarity() missing 1 required positional argument: 'chunckIdx'"
     ]
    }
   ],
   "source": [
    "graph_article = create_graph(selected_postings, \"ID_Article\", \"UserCommunityName\")\n",
    "uu_overlap_article = compute_overlap(graph_article, selected_postings, \"ID_Article\")\n",
    "user_num_articles = user_lookup_df(selected_postings, \"ID_Article\")\n",
    "similarity_table_article = compute_similarity(uu_overlap_article, user_num_articles)\n",
    "similarity_table_article.to_csv(\"output/similarity_table_article_all.csv\", index=False)\n",
    "del similarity_table_article, graph_article, uu_overlap_article, user_num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_base_similiarities(selected_postings, article_or_ressort, num_chunks=30):\n",
    "    #chunks = []\n",
    "    for chunckIdx, subset_df  in enumerate(np.array_split(selected_postings,num_chunks)):\n",
    "        print(round(chunckIdx/num_chunks) *100, \" %\", end=\"\\r\")\n",
    "        graph_ressort = create_graph(subset_df, article_or_ressort, \"UserCommunityName\")\n",
    "        uu_overlap_ressort = compute_overlap(graph_ressort, subset_df, article_or_ressort)\n",
    "        user_num_article_or_ressort = user_lookup_df(subset_df, article_or_ressort)\n",
    "        similarity_table_ressort = compute_similarity(uu_overlap_ressort, user_num_article_or_ressort,chunckIdx)\n",
    "        #chunks += [similarity_table_ressort]\n",
    "        yield similarity_table_ressort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  %\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "time_similarity_df = compute_time_base_similiarities(selected_postings, \"ID_Article\",15)\n",
    "time_similarity_df = pd.concat(time_similarity_df, axis=1)\n",
    "time_similarity_df.to_csv(\"output/time_similarity_table_ressort.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
