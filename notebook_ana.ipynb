{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.load_data import load_postings, load_votes, get_first_contact_df, subset_users\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = load_votes(\"input/\")\n",
    "postings = load_postings(\"input/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Ana doing things that got done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_categories = {}\n",
    "# for user in postings_sub['ID_CommunityIdentity'].unique():\n",
    "    \n",
    "#     user_categories[user] = set(postings_sub.query(\"ID_CommunityIdentity == @user\")['ArticleRessortName'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaqquard_similarity(a, b):\n",
    "#     return len(a & b) / len(a | b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# groups = postings_sub.groupby(\"ID_CommunityIdentity\")\n",
    "\n",
    "# all_groups = groups.groups.keys()\n",
    "# groups_list = list(all_groups)\n",
    "# list_len = len(groups_list)\n",
    "\n",
    "# df = pd.DataFrame(columns=('UserA', 'UserB', 'topics_num'))\n",
    "\n",
    "# for i in range (list_len):\n",
    "#     for j in range(i+1, list_len):\n",
    "#         id_a = groups_list[i]\n",
    "#         id_b = groups_list[j]\n",
    "#         group_a = groups.get_group(id_a)\n",
    "#         group_b = groups.get_group(id_b)\n",
    "#         topics_num = len(np.intersect1d(group_a.ArticleRessortName, group_b.ArticleRessortName))\n",
    "\n",
    "#         print(str(id_a)+ \", \"+ str(id_b)+ \": \"+ str(topics_num))\n",
    "\n",
    "#         df.append({'UserA': id_a, 'UserB': id_b, 'topics_num': topics_num}, ignore_index=True)\n",
    "\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_resort = {}\n",
    "# user_channel = {}\n",
    "# for user in postings['ID_CommunityIdentity'].unique():\n",
    "    \n",
    "#     user_resort[user] = set(postings.query(\"ID_CommunityIdentity == @user\")['ArticleRessortName'].values)\n",
    "#     user_channel[user] = set(postings.query(\"ID_CommunityIdentity == @user\")['ArticleChannel'].values)\n",
    "# arr = []\n",
    "# users = list(user_channel.keys())\n",
    "# alfa = 0.3\n",
    "\n",
    "# for user_a in tqdm(users):\n",
    "#     user_channel_a = user_channel[user_a]\n",
    "#     user_resort_a = user_resort[user_a]\n",
    "    \n",
    "#     for user_b in users:\n",
    "#         if user_a != user_b:\n",
    "#             user_channel_b = user_channel[user_b]\n",
    "#             user_resort_b = user_resort[user_b]\n",
    "\n",
    "#             similarity_channel = jaqquard_similarity(user_channel_a, user_channel_b)\n",
    "#             similarity_resort = jaqquard_similarity(user_resort_a, user_resort_b)\n",
    "#             weighted_similarity = alfa * similarity_channel + (1 - alfa) * similarity_resort\n",
    "\n",
    "#             arr.append([user_a, user_b, weighted_similarity])\n",
    "\n",
    "# arr\n",
    "\n",
    "# pd.DataFrame(arr, columns=[\"A\", \"B\", \"Similarity\"]).set_index([\"A\", \"B\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "playing with weighted similarity for channel and resort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "######the code below completly taken from notebook.ipynb ##################################\n",
    "\n",
    "import itertools\n",
    "# one can adapt this with e.g. ressort instead of article\n",
    "\n",
    "def create_graph(df, article_or_ressort = \"ID_Posting\", user=\"UserVote\"):\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(df[article_or_ressort].unique())\n",
    "    graph.add_nodes_from(df[user].unique())\n",
    "    graph.add_edges_from(list(map(tuple, df[[article_or_ressort, user]].drop_duplicates().values)))\n",
    "    graph = graph.to_undirected()\n",
    "    return graph\n",
    "\n",
    "def compute_overlap(graph, df, article_or_ressort,verbose=False):\n",
    "    uu_overlap = {}\n",
    "    article_ids = df[article_or_ressort].unique()\n",
    "    for idx, article in enumerate(article_ids):\n",
    "        if verbose: print(round((idx/len(article_ids))*100), \"%\", end=\"\\r\")\n",
    "        users_commented =list(graph.neighbors(article))\n",
    "        for uu_tuple in itertools.product(users_commented, users_commented):\n",
    "            if uu_tuple[0] != uu_tuple[1]:\n",
    "                if uu_tuple[0] > uu_tuple[1]:\n",
    "                    uu_tuple = (uu_tuple[1], uu_tuple[0])\n",
    "                if uu_tuple in uu_overlap:\n",
    "                    uu_overlap[uu_tuple] += 1\n",
    "                else:\n",
    "                    uu_overlap[uu_tuple] = 1\n",
    "                    \n",
    "    return uu_overlap\n",
    "\n",
    "\n",
    "def user_lookup_df(df, article_or_ressort=\"ID_Posting\"):\n",
    "    user_num_articles = df[[\"UserVote\", article_or_ressort]].drop_duplicates()\\\n",
    "        .groupby([\"UserVote\"]).size().to_frame()\n",
    "    # make dict of users and the number of articles they voted on\n",
    "    user_num_articles = dict(zip(user_num_articles.index, user_num_articles[0]))\n",
    "    return user_num_articles\n",
    "\n",
    "def compute_similarity(uu_overlap, user_num_articles, chunckIdx):\n",
    "    similarities = []\n",
    "    for uu_tuple in uu_overlap.keys():\n",
    "        overlap = uu_overlap[uu_tuple]\n",
    "        try:\n",
    "            union = user_num_articles[uu_tuple[0]] + user_num_articles[uu_tuple[1]] \n",
    "        except:\n",
    "            print(uu_tuple)\n",
    "        similarities += [[uu_tuple[0],uu_tuple[1], overlap/union ]]\n",
    "    return pd.DataFrame(similarities, columns=[\"A\", \"B\", f\"Similarity_{chunckIdx}\"]).set_index([\"A\", \"B\"])\n",
    "\n",
    "def compute_time_base_similiarities(selected_postings, article_or_ressort, num_chunks=30):\n",
    "    sum_sims= 0\n",
    "    n = 0\n",
    "    chunks = []\n",
    "    running_mean = []\n",
    "    for chunckIdx, subset_df  in enumerate(np.array_split(selected_postings,num_chunks)):\n",
    "        print(round(chunckIdx/num_chunks) *100, \" %\", end=\"\\r\")\n",
    "        graph_ressort = create_graph(subset_df, article_or_ressort, \"UserCommunityName\")\n",
    "        uu_overlap_ressort = compute_overlap(graph_ressort, subset_df, article_or_ressort)\n",
    "        user_num_article_or_ressort = user_lookup_df(subset_df, article_or_ressort)\n",
    "        similarity_table_ressort = compute_similarity(uu_overlap_ressort, user_num_article_or_ressort,chunckIdx)\n",
    "        n += similarity_table_ressort.shape[0]\n",
    "        sum_sims += similarity_table_ressort.sum().item()\n",
    "        chunks += [similarity_table_ressort.mean()]\n",
    "        running_mean += [sum_sims / n]\n",
    "    return chunks, running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_selection = subset_users(votes, postings, \"both\", num_days_min=30, firt_interaction_middle=True)\n",
    "\n",
    "selected_postings = postings[postings[\"UserCommunityName\"].isin(user_selection)].sort_values(\"PostingCreatedAt\")\n",
    "selected_postings[\"UserCommunityName\"] = \"user_\" + selected_postings[\"UserCommunityName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity based on article topic\n",
    "time_similarity_channel, time_similarity_running_channel = compute_time_base_similiarities(selected_postings, \"ArticleRessortName\")\n",
    "time_similarity_ressort, time_similarity_running_ressort = compute_time_base_similiarities(selected_postings, \"ArticleChannel\")\n",
    "\n",
    "postings['ArticleRessortName'].nunique()\n",
    "postings['ArticleChannel'].nunique()\n",
    "alfa = (postings['ArticleChannel'].nunique())/(postings['ArticleChannel'].nunique()+postings['ArticleRessortName'].nunique())\n",
    "print(alfa)\n",
    "# alfa = 0.3\n",
    "\n",
    "time_similarity_channel_alfa = [x * alfa for x in time_similarity_channel]\n",
    "time_similarity_ressort_alfa = [x * (1-alfa) for x in time_similarity_ressort]\n",
    "\n",
    "time_similarity_running_channel_alfa = [x * alfa for x in time_similarity_running_channel]\n",
    "time_similarity_running_ressort_alfa = [x * (1-alfa) for x in time_similarity_running_ressort]\n",
    "\n",
    "\n",
    "time_similarity_final = [x + y for x, y in zip([x * alfa for x in time_similarity_channel], [x * (1-alfa) for x in time_similarity_ressort])]\n",
    "time_similarity_running_final = [x + y for x, y in zip([x * alfa for x in time_similarity_running_channel], [x * (1-alfa) for x in time_similarity_running_ressort])]\n",
    "\n",
    "plt.plot(range(1,31), time_similarity_final)\n",
    "plt.plot(range(1,31), time_similarity_running_final)\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces, one for each slider step\n",
    "for alfa in np.arange(0, 1.1, 0.01):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#00CED1\", width=6),\n",
    "            name=\"𝜈 = \" + str(alfa),\n",
    "            x=np.arange(1, 31, 1),\n",
    "            y = [x + y for x, y in zip([x * alfa for x in time_similarity_running_channel], [x * (1-alfa) for x in time_similarity_running_ressort])]\n",
    "\n",
    "))\n",
    "\n",
    "# Make 10th trace visible\n",
    "fig.data[10].visible = True\n",
    "\n",
    "# Create and add slider\n",
    "steps = []\n",
    "for i in np.arange(0, 1.01, 0.01):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)},\n",
    "              {\"title\": \"Slider switched to alfa: \" + str(i)}],  # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][int(i/0.01)] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=0.5,\n",
    "    currentvalue={\"prefix\": \"Alfa: \"},\n",
    "    steps=steps,\n",
    "    \n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votes similarity\n",
    "adjusted functions from notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(votes, postings, on=\"ID_Posting\")\n",
    "merged[\"Vote\"] = merged[\"VotePositive\"] - merged[\"VoteNegative\"]\n",
    "merged = merged.rename(columns={\"UserCommunityName_x\": \"UserVote\", \"UserCommunityName_y\": \"UserPost\"})\n",
    "merged = merged[merged[\"UserVote\"].isin(user_selection)].sort_values(\"VoteCreatedAt\")\n",
    "merged = merged[[\"ID_Posting\", \"UserPost\", \"UserVote\", \"Vote\", \"VoteCreatedAt\"]]\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_votes(graph_pos, graph_neg, df, article_or_ressort=\"ID_Posting\",verbose=False):\n",
    "    uu_overlap = {}\n",
    "    article_ids = df[article_or_ressort].unique()\n",
    "    for idx, article in enumerate(article_ids):\n",
    "\n",
    "        try:\n",
    "            users_voted_pos =list(graph_pos.neighbors(article))\n",
    "            for uu_tuple in itertools.product(users_voted_pos, users_voted_pos):\n",
    "                if uu_tuple[0] != uu_tuple[1]:\n",
    "                    if uu_tuple[0] > uu_tuple[1]:\n",
    "                        uu_tuple = (uu_tuple[1], uu_tuple[0])\n",
    "                    if uu_tuple in uu_overlap:\n",
    "                        uu_overlap[uu_tuple] += 1\n",
    "                    else:\n",
    "                        uu_overlap[uu_tuple] = 1\n",
    "        except:\n",
    "            users_voted_pos = []\n",
    "\n",
    "        try:\n",
    "            users_voted_neg =list(graph_neg.neighbors(article))\n",
    "            for uu_tuple in itertools.product(users_voted_neg, users_voted_neg):\n",
    "                if uu_tuple[0] != uu_tuple[1]:\n",
    "                    if uu_tuple[0] > uu_tuple[1]:\n",
    "                        uu_tuple = (uu_tuple[1], uu_tuple[0])\n",
    "                    if uu_tuple in uu_overlap:\n",
    "                        uu_overlap[uu_tuple] += 1\n",
    "                    else:\n",
    "                        uu_overlap[uu_tuple] = 1\n",
    "        except:\n",
    "            users_voted_neg = []\n",
    "\n",
    "        if (len(users_voted_neg) != 0 & len(users_voted_pos) != 0):\n",
    "            for uu_tuple in itertools.product(users_voted_pos, users_voted_neg):\n",
    "                if uu_tuple[0] != uu_tuple[1]:\n",
    "                    if uu_tuple[0] > uu_tuple[1]:\n",
    "                        uu_tuple = (uu_tuple[1], uu_tuple[0])\n",
    "                    if uu_tuple in uu_overlap:\n",
    "                        uu_overlap[uu_tuple] -= 1\n",
    "                    else:\n",
    "                        uu_overlap[uu_tuple] = -1\n",
    "                    \n",
    "            \n",
    "    return uu_overlap\n",
    "\n",
    "\n",
    "def compute_time_base_similiarities_votes(merged, article_or_ressort=\"ID_Posting\", num_chunks=30):\n",
    "    sum_sims= 0\n",
    "    n = 0\n",
    "    chunks = []\n",
    "    running_mean = []\n",
    "    for chunckIdx, subset_df  in tqdm(enumerate(np.array_split(merged,num_chunks))):\n",
    "        print(round(chunckIdx/num_chunks) *100, \" %\", end=\"\\r\")\n",
    "        positive = subset_df[subset_df[\"Vote\"] == 1]\n",
    "        negative = subset_df[subset_df[\"Vote\"] == -1]\n",
    "        graph_pos = create_graph(positive, article_or_ressort, \"UserVote\")\n",
    "        graph_neg = create_graph(negative, article_or_ressort, \"UserVote\")\n",
    "        uu_overlap_ressort = compute_overlap_votes(graph_pos, graph_neg, subset_df, article_or_ressort)\n",
    "        user_num_article_or_ressort = user_lookup_df(subset_df, article_or_ressort)\n",
    "        similarity_table_ressort = compute_similarity(uu_overlap_ressort, user_num_article_or_ressort,chunckIdx)\n",
    "        n += similarity_table_ressort.shape[0]\n",
    "        sum_sims += similarity_table_ressort.sum().item()\n",
    "        chunks += [similarity_table_ressort.mean()]\n",
    "        running_mean += [sum_sims / n]\n",
    "    return chunks, running_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_similarity, time_similarity_running = compute_time_base_similiarities_votes(merged, \"ID_Posting\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.annotate(f'First Contact (replies or votes)', xy=(15, 0.5), xytext=(1, 0.52),\n",
    "                    arrowprops=dict(facecolor='black', shrink=0.05, width=2, headwidth=7))\n",
    "\n",
    "plt.title(f'Similarity over time (Votes)')\n",
    "plt.xlabel(\"days\")\n",
    "plt.ylabel(\"Similarity\")\n",
    "plt.plot(range(1, 31), time_similarity, alpha=0.1, color=\"blue\")\n",
    "plt.plot(range(1, 31), time_similarity_running, color=\"green\")\n",
    "plt.legend([\"Actual values\", \"Running mean\"])\n",
    "plt.axvline(x=15, color=\"green\", linestyle=\"--\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df87b94f50c6b8081a9a425f6ade537f06441a824b7d68371eb29c5a5eb2838b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
